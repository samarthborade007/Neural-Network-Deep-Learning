{"cells":[{"cell_type":"code","execution_count":13,"metadata":{"id":"whyChvCRyfYT"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Input, Dense, Flatten \n","from tensorflow.keras.models import Model\n","from tensorflow.keras.applications.vgg16 import VGG16\n","from tensorflow.keras.applications.vgg16 import preprocess_input\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.preprocessing import image_dataset_from_directory\n","import cv2\n","import numpy as np\n","import os\n","import pandas as pd"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["from keras.applications import VGG16\n","import ssl\n","\n","# Workaround to avoid SSL certificate verification error\n","ssl._create_default_https_context = ssl._create_unverified_context"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"gHKl1405y7Pi"},"outputs":[],"source":["# VGG model requires 224*224 input so we are going to re-size all images\n","IMAGE_SIZE = [224, 224]"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"WRBEb2sAzi-9"},"outputs":[],"source":["#file_path=\"../TEST/\""]},{"cell_type":"code","execution_count":17,"metadata":{"id":"FyNTLMIEzqen","outputId":"0fc6346e-2c06-42a9-9a01-ea38e5e6a98a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 59922 images belonging to 2 classes.\n","Found 16726 images belonging to 2 classes.\n"]}],"source":["# example of progressively loading images from file\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","# create generator\n","datagen = ImageDataGenerator(rescale = 1./255,\n","                             shear_range = 0.2,\n","                             zoom_range = 0.2,\n","                             horizontal_flip = True,\n","                             vertical_flip=True,\n","                             rotation_range=30)\n","# prepare an iterators for each dataset\n","train_it = datagen.flow_from_directory('./TRAIN.1', \n","                                       class_mode='categorical',\n","                                       target_size=(224, 224),\n","                                       batch_size=2,\n","                                       seed=7)\n","\n","test_it = datagen.flow_from_directory('./TEST',  \n","                                       class_mode='categorical',\n","                                       target_size=(224, 224),\n","                                       batch_size=2,\n","                                       seed=7)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"ulsvDJgx0O4v"},"outputs":[],"source":["from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","from tensorflow.keras import Sequential\n"]},{"cell_type":"markdown","metadata":{"id":"-bC1KtHrNn5F"},"source":["## Using VGG16 model"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"qGSf1SrK3lt8"},"outputs":[],"source":["# VGG16 model"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"lv-oMlgG-O7_"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58889256/58889256 [==============================] - 31s 1us/step\n"]}],"source":["vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"3oLjw-iW-aOj"},"outputs":[],"source":["for layer in vgg.layers:\n","  layer.trainable = False"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"WB7hYJpa-dLo"},"outputs":[],"source":["output_classes = 2"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"TxO_VdRb-gQz"},"outputs":[],"source":["##Adding flatten and Dense Layer\n","x = Flatten()(vgg.output)\n","x = Dense(1000, activation='relu')(x)\n","\n","prediction = Dense(output_classes, activation='softmax')(x)"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"esXRaMFl-jkH"},"outputs":[],"source":["# create a model object\n","model = Model(inputs=vgg.input, outputs=prediction)"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"zfBxzhl1-lhz","outputId":"03c20658-0b60-46fb-fbd5-ce4c3e487d22"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n","                                                                 \n"," block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n","                                                                 \n"," block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n","                                                                 \n"," block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n","                                                                 \n"," block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n","                                                                 \n"," block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n","                                                                 \n"," block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n","                                                                 \n"," block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n","                                                                 \n"," block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n","                                                                 \n"," block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n","                                                                 \n"," block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n","                                                                 \n"," block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n","                                                                 \n"," flatten (Flatten)           (None, 25088)             0         \n","                                                                 \n"," dense (Dense)               (None, 1000)              25089000  \n","                                                                 \n"," dense_1 (Dense)             (None, 2)                 2002      \n","                                                                 \n","=================================================================\n","Total params: 39805690 (151.85 MB)\n","Trainable params: 25091002 (95.71 MB)\n","Non-trainable params: 14714688 (56.13 MB)\n","_________________________________________________________________\n"]}],"source":["# view the structure of the model\n","model.summary()"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"jMyZTiLE-nfL"},"outputs":[],"source":["# tell the model what cost and optimization method to use\n","from tensorflow.keras.losses import CategoricalCrossentropy\n","model.compile(\n","  loss=CategoricalCrossentropy(),\n","  optimizer='adam',\n","  metrics=['accuracy']\n",")"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"s_dyFpTP-qXP","outputId":"b5bd9a86-580c-409b-c647-cf3ff77cd5a1"},"outputs":[{"name":"stdout","output_type":"stream","text":["29961/29961 [==============================] - 2301s 77ms/step - loss: 3.6827 - accuracy: 0.8930 - val_loss: 5.3279 - val_accuracy: 0.8492\n"]}],"source":["history_vgg= model.fit(\n","  train_it,\n","  validation_data=test_it,\n","  epochs=1 # keep epochs=5 if you want to just check as it take more computational time\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lhl0HVbqM8LC"},"outputs":[],"source":["##SAVING A MODEL"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"YhNVPvkA-whC"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]}],"source":["model.save(\"bio.h5\")"]},{"cell_type":"markdown","metadata":{"id":"dt5Oz7IePwsz"},"source":["##If model is trained already,Then  Load model"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"NdMVKs4xaRZ9","outputId":"32f09c86-6609-4937-e88f-edc2bd3fa367"},"outputs":[],"source":["from keras.models import load_model\n"," \n","# load model\n","model = load_model('bio.h5')"]},{"cell_type":"markdown","metadata":{},"source":["## Checking model with external image"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"vAuXPy-eZL3d"},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 269ms/step\n"]}],"source":["\n","from keras.preprocessing.image import load_img\n","image = load_img('./tomato.jpeg', target_size=(224, 224))\n","img = np.array(image)\n","img = img / 255.0\n","img = img.reshape(1,224,224,3)\n","label = model.predict(img)"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"xTRrB6VsZ_GK","outputId":"5d817ebb-b460-4d46-ab88-2c37564067e6"},"outputs":[],"source":["l={\"Biodegradable\":label[0][0],\"NON-Biodegradable\":label[0][1]}\n","def get_key(val):\n","    for key, value in l.items():\n","         if val == value:\n","             return key\n"," \n","    return \"key doesn't exist\"\n","#label[0][1]"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"wF1u8MEQaCjB"},"outputs":[{"data":{"text/plain":["'Biodegradable'"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["check=label.max()\n","get_key(check)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"psmQdX5kaVkc"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xK_uSPZVaZf5"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y986Z_h0bImg"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":4}
